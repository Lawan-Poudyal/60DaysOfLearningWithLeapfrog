# Day 29 

## Mini-batch gradient descent:
- It makes computation faster by making training in smaller sets

## Choosing mini-batches size:
- If training set is too small, use batch gradient descent
- else typically they are in power of 2 (ex 64, 256, 521)